import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import numpy as np
import pandas as pd
from pathlib import Path
import gc

from config import Config
from dataset import get_dataloaders
from models import BiomassPredictor
from utils import (
    set_seed,
    weighted_r2_score,
    WeightedR2Loss,
    ConsistencyLoss,
    AverageMeter,
    EarlyStopping,
)


class Trainer:
    def __init__(self, config):
        self.config = config
        self.device = config.DEVICE

        # è®¾ç½®éšæœºç§å­
        set_seed(config.SEED)

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = BiomassPredictor(
            model_name=config.MODEL_NAME,
            num_tabular_features=10,  # æ ¹æ®dataset.pyä¸­çš„ç‰¹å¾æ•°
            pretrained=config.PRETRAINED,
            dropout_rate=0.4,
        ).to(self.device)

        # æŸå¤±å‡½æ•°
        self.r2_loss = WeightedR2Loss(config.TARGET_WEIGHTS)
        self.consistency_loss = ConsistencyLoss(weight=0.15)

        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY,
        )

        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=10,  # 10ä¸ªepochåç¬¬ä¸€æ¬¡é‡å¯
            T_mult=2,  # æ¯æ¬¡é‡å¯å‘¨æœŸç¿»å€
            eta_min=1e-6,
        )

        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler()

        # Early stopping
        self.early_stopping = EarlyStopping(
            patience=config.PATIENCE, min_delta=config.MIN_DELTA, mode="max"
        )

        # æœ€ä½³æ¨¡å‹è¿½è¸ª
        self.best_score = -np.inf

        # DataLoaders
        self.train_loader, self.val_loader = get_dataloaders(config)

        print(f"Training on fold {config.FOLD}/{config.N_FOLDS}")
        print(f"Train samples: {len(self.train_loader.dataset)}")
        print(f"Val samples: {len(self.val_loader.dataset)}")
        print(f"Device: {self.device}")

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()

        # æŸå¤±è¿½è¸ª
        r2_losses = AverageMeter()
        consistency_losses = AverageMeter()
        total_losses = AverageMeter()

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.config.NUM_EPOCHS}")

        for batch_idx, batch in enumerate(pbar):
            images = batch["image"].to(self.device)
            tabular = batch["tabular"].to(self.device)
            targets = {k: v.to(self.device) for k, v in batch["targets"].items()}

            # å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
            with autocast():
                predictions = self.model(images, tabular)

                # è®¡ç®—æŸå¤±
                r2_loss = self.r2_loss(predictions, targets)
                consistency_loss = self.consistency_loss(predictions)

                total_loss = r2_loss + consistency_loss

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            self.scaler.scale(total_loss).backward()

            # æ¢¯åº¦è£å‰ª
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.scaler.step(self.optimizer)
            self.scaler.update()

            # æ›´æ–°è¿½è¸ª
            batch_size = images.size(0)
            r2_losses.update(r2_loss.item(), batch_size)
            consistency_losses.update(consistency_loss.item(), batch_size)
            total_losses.update(total_loss.item(), batch_size)

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix(
                {
                    "r2_loss": f"{r2_losses.avg:.4f}",
                    "cons_loss": f"{consistency_losses.avg:.4f}",
                    "total": f"{total_losses.avg:.4f}",
                    "lr": f'{self.optimizer.param_groups[0]["lr"]:.6f}',
                }
            )

        return {
            "r2_loss": r2_losses.avg,
            "consistency_loss": consistency_losses.avg,
            "total_loss": total_losses.avg,
        }

    @torch.no_grad()
    def validate(self, epoch):
        """éªŒè¯"""
        self.model.eval()

        # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å€¼
        all_predictions = {name: [] for name in self.config.TARGET_NAMES}
        all_targets = {name: [] for name in self.config.TARGET_NAMES}

        r2_losses = AverageMeter()

        pbar = tqdm(self.val_loader, desc=f"Validation")

        for batch in pbar:
            images = batch["image"].to(self.device)
            tabular = batch["tabular"].to(self.device)
            targets = {k: v.to(self.device) for k, v in batch["targets"].items()}

            # å‰å‘ä¼ æ’­
            predictions = self.model(images, tabular)

            # è®¡ç®—æŸå¤±
            r2_loss = self.r2_loss(predictions, targets)
            r2_losses.update(r2_loss.item(), images.size(0))

            # æ”¶é›†é¢„æµ‹ç»“æœï¼ˆè½¬åˆ°CPUï¼‰
            for target_name in self.config.TARGET_NAMES:
                all_predictions[target_name].extend(
                    predictions[target_name].cpu().numpy()
                )
                all_targets[target_name].extend(targets[target_name].cpu().numpy())

            pbar.set_postfix({"r2_loss": f"{r2_losses.avg:.4f}"})

        # è®¡ç®—åŠ æƒRÂ²åˆ†æ•°
        weighted_r2 = weighted_r2_score(
            all_targets, all_predictions, self.config.TARGET_WEIGHTS
        )

        # è®¡ç®—æ¯ä¸ªtargetçš„RÂ²ï¼ˆç”¨äºåˆ†æï¼‰
        individual_r2 = {}
        for target_name in self.config.TARGET_NAMES:
            from sklearn.metrics import r2_score

            r2 = r2_score(all_targets[target_name], all_predictions[target_name])
            individual_r2[target_name] = r2

        print(f"\nValidation Results (Epoch {epoch+1}):")
        print(f"Weighted RÂ²: {weighted_r2:.4f}")
        print("Individual RÂ² scores:")
        for name, score in individual_r2.items():
            weight = self.config.TARGET_WEIGHTS[name]
            print(f"  {name:20s}: {score:.4f} (weight={weight})")

        return {
            "weighted_r2": weighted_r2,
            "individual_r2": individual_r2,
            "r2_loss": r2_losses.avg,
        }

    def save_checkpoint(self, epoch, val_metrics, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict(),
            "scaler_state_dict": self.scaler.state_dict(),
            "val_metrics": val_metrics,
            "config": self.config.__dict__,
        }

        # ä¿å­˜æœ€æ–°çš„checkpoint
        checkpoint_path = (
            self.config.OUTPUT_DIR / f"checkpoint_fold{self.config.FOLD}_latest.pth"
        )
        torch.save(checkpoint, checkpoint_path)

        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜
        if is_best:
            best_path = (
                self.config.OUTPUT_DIR / f"model_fold{self.config.FOLD}_best.pth"
            )
            torch.save(checkpoint, best_path)
            print(f"Saved best model to {best_path}")

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "=" * 50)
        print("Starting Training")
        print("=" * 50 + "\n")

        for epoch in range(self.config.NUM_EPOCHS):
            print(f"\nEpoch {epoch+1}/{self.config.NUM_EPOCHS}")
            print("-" * 50)

            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)

            # éªŒè¯
            val_metrics = self.validate(epoch)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()

            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            current_score = val_metrics["weighted_r2"]
            is_best = current_score > self.best_score

            if is_best:
                self.best_score = current_score
                print(f"\nğŸ‰ New best score: {self.best_score:.4f}")

            # ä¿å­˜checkpoint
            self.save_checkpoint(epoch, val_metrics, is_best=is_best)

            # Early stopping
            if self.early_stopping(current_score):
                print(f"\nEarly stopping triggered at epoch {epoch+1}")
                break

            # æ¸…ç†å†…å­˜
            gc.collect()
            torch.cuda.empty_cache()

        print("\n" + "=" * 50)
        print(f"Training completed!")
        print(f"Best weighted RÂ²: {self.best_score:.4f}")
        print("=" * 50 + "\n")

        return self.best_score


def main():
    config = Config()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # è®­ç»ƒ
    trainer = Trainer(config)
    best_score = trainer.train()

    print(f"\nFold {config.FOLD} completed with best score: {best_score:.4f}")


if __name__ == "__main__":
    main()
